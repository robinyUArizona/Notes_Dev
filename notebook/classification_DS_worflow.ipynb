{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML DS general workflow example  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "First, import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from word2number import w2n\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from category_encoders import CountEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Train and Test Datasets\n",
    "Load the `train.csv` and `test.csv` files into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv('train.csv')  # Replace with your actual training data file\n",
    "test_data = pd.read_csv('test.csv')  # Replace with your actual test data file\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define a Function to Convert Words to Numbers\n",
    "Define a function `convert_to_float` to handle word-to-number conversion and remove negative signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert numeric words to float and remove negative signs\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        num = float(value)  # Convert directly if it's a number\n",
    "    except ValueError:\n",
    "        num = float(w2n.word_to_num(value))  # Convert word to number\n",
    "    return abs(num)  # Remove negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess the column in Train and Test Data\n",
    "Split the column into two new columns (col1 and col2), convert their values to floats, and remove negative signs.\n",
    "##### Sample DataFrame\n",
    "`data_col = {'column': ['-66.0; 2', 'sixty-two, -3', '-seventy-four: 4', '90_5']}`\n",
    "\n",
    "\n",
    "`df_col = pd.DataFrame(data_col)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the 'column'\n",
    "def preprocess_column(df):\n",
    "    # Split into two columns based on multiple delimiters (;, :, ,, _)\n",
    "    df[['col1', 'col2']] = df['column'].str.split(r'[;:,_]\\s*', expand=True)\n",
    "\n",
    "    # Convert col1 and col2 values to float and remove negative signs\n",
    "    df['col1'] = df['col1'].apply(convert_to_float)\n",
    "    df['col2'] = df['col2'].astype(float).abs()  # Convert col2 to float and remove negative signs\n",
    "\n",
    "    # Drop the original column\n",
    "    df.drop(columns=['column'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Preprocess the train and test datasets\n",
    "train_data = preprocess_column(train_data)\n",
    "test_data = preprocess_column(test_data)\n",
    "\n",
    "# Display the processed train dataset\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Separate Features and Target\n",
    "Separate the features (X) and target (y) for the training dataset. For the test dataset, only features are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Features (X) and Target (y) for train dataset\n",
    "X_train = train_data.drop(columns=['Target'])  # Assuming 'Target' is the column name for the target\n",
    "y_train = train_data['Target']\n",
    "\n",
    "# Separate Features (X) for test dataset\n",
    "X_test = test_data.drop(columns=['Target'], errors='ignore')  # Drop 'Target' if it exists in test_data\n",
    "\n",
    "# Display the first few rows of X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Automatically Identify Numerical and Categorical Features\n",
    "Identify numerical and categorical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically select numerical and categorical columns\n",
    "numerical_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Features: {numerical_features}\")\n",
    "print(f\"Categorical Features: {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create a Preprocessing Pipeline\n",
    "Create a preprocessing pipeline for numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a column transformer\n",
    "def create_column_transformer(numerical_features, categorical_features):\n",
    "    # Preprocessing for numerical features\n",
    "    num_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),  # Impute missing values with median\n",
    "        RobustScaler()  # Scale numerical features\n",
    "    )\n",
    "    \n",
    "    # Preprocessing for categorical features\n",
    "    freq_encoder = CountEncoder(normalize=True)  # Frequency Encoding\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')  # One-Hot Encoding\n",
    "\n",
    "    cat_transformer = make_column_transformer(\n",
    "        (freq_encoder, categorical_features),  # Frequency Encoding for categorical features\n",
    "        (one_hot_encoder, categorical_features),  # One-Hot Encoding for categorical features\n",
    "        remainder=\"drop\"  # Drop columns not explicitly transformed\n",
    "    )\n",
    "\n",
    "    column_transformer = make_column_transformer(\n",
    "        (num_transformer, numerical_features),  # Apply numerical transformer to numerical features\n",
    "        (cat_transformer, categorical_features),  # Apply categorical transformer to categorical features\n",
    "        remainder=\"drop\"  # Drop columns not explicitly transformed\n",
    "    )\n",
    "    \n",
    "    return column_transformer\n",
    "\n",
    "# Initialize the column transformer\n",
    "column_transformer = create_column_transformer(numerical_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Transform the Training and Test Data\n",
    "Apply the preprocessing pipeline to the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "input_features_train_array = column_transformer.fit_transform(X_train).toarray()\n",
    "print(\"Encoded and Scaled Train dataset:\")\n",
    "pd.DataFrame(input_features_train_array).head()\n",
    "\n",
    "# Transform the test data\n",
    "input_features_test_array = column_transformer.transform(X_test).toarray()\n",
    "print(\"Encoded and Scaled Test dataset:\")\n",
    "pd.DataFrame(input_features_test_array).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Define and Evaluate Models\n",
    "Define three models (RandomForest, LogisticRegression, and XGBoost) and evaluate them using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models using cross-validation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, input_features_train_array, y_train, cv=5, scoring='accuracy')\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "    results[name] = mean_cv_score\n",
    "    print(f'{name} Cross-Validation Accuracy: {mean_cv_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Train the Best Model and Make Predictions\n",
    "Select the best model, train it on the full training set, and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_model.fit(input_features_train_array, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(input_features_test_array)\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'outcome': test_predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Hyperparameter Tuning (Optional)\n",
    "If you want to tune the hyperparameters of a specific model (e.g., RandomForest), you can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for RandomForest\n",
    "tuned_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "tuned_model.fit(input_features_train_array, y_train)\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "tuned_predictions = tuned_model.predict(input_features_test_array)\n",
    "\n",
    "# Evaluate accuracy (if true labels are available)\n",
    "accuracy = accuracy_score(y_test, tuned_predictions)\n",
    "print(f'Tuned Random Forest Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
